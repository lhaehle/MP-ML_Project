{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0e6ff75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "import time\n",
    "import pdb\n",
    "import bdb  # Added to handle BdbQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79a8f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 1: # Create SQLAlchemy engine\n",
    "from dotenv import load_dotenv\nimport os\nload_dotenv()\n\nconnection_string = (\n    f\"mssql+pyodbc://{os.getenv('DB_USER')}:{os.getenv('DB_PASSWORD')}\"\n    f\"@{os.getenv('DB_SERVER')}:{os.getenv('DB_PORT')}/{os.getenv('DB_NAME')}\"\n    f\"?driver=ODBC+Driver+17+for+SQL+Server\"\n    f\"&TrustServerCertificate=yes\"\n    f\"&Connection+Timeout=5\"\n)\n",
    "    engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5e0e06ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows fetched: 30,625,458\n",
      "  REPORT_YEAR_WEEK INV_STATE_ABBRV INV_TOWN_NAME  DAYS_LSTD_BFR_SLD  \\\n",
      "0          2025-42              CA    SANTA ROSA             -999.0   \n",
      "1          2025-36              SC         GREER             -999.0   \n",
      "2          2025-38              TX        DALLAS               62.0   \n",
      "3          2025-41              TX         NIXON             -999.0   \n",
      "4          2025-37              MT      MISSOULA             -999.0   \n",
      "\n",
      "  SRC_VEH_FIRST_SCRAPED_DT TRIM_DESCRIPTION NVI_OWNSHP_DT  NVI_EFCTV_START_DT  \\\n",
      "0               2025-08-05               SE    1899-12-30 1899-12-30 00:00:00   \n",
      "1               2024-11-14   1500 ELEVATION    1899-12-30 1899-12-30 00:00:00   \n",
      "2               2025-07-30         350 BASE    2025-10-08 2025-11-04 20:41:07   \n",
      "3               2025-09-28       150 TREMOR    1899-12-30 1899-12-30 00:00:00   \n",
      "4               2025-07-12        PREFERRED    1899-12-30 1899-12-30 00:00:00   \n",
      "\n",
      "  NVI_CENSUS_TRACT NVI_RPT_YYYYMM  ... SLS_STATE_ABBRV NVI_CONTROL_NBR  \\\n",
      "0                                  ...                                   \n",
      "1                                  ...                                   \n",
      "2      48113019301         999912  ...                         0169464   \n",
      "3                                  ...                                   \n",
      "4                                  ...                                   \n",
      "\n",
      "  NVI_DEALER_NAME NVI_TOWN_NAME NVI_STATE_ABBRV   MAKE_DESC MODEL_DESC  \\\n",
      "0                                                VOLKSWAGEN        GTI   \n",
      "1                                                       GMC     SIERRA   \n",
      "2    SEWELL LEXUS        DALLAS              TX       LEXUS         TX   \n",
      "3                                                      FORD   F SERIES   \n",
      "4                                                     MAZDA  UNDEFINED   \n",
      "\n",
      "  SERIES_TEXT                SEGMENT_DESC SLS_VEHICLE_COUNT  \n",
      "0          SE                *COMPACT CAR               0.0  \n",
      "1         15P                LARGE PICKUP               0.0  \n",
      "2         350              MID LUXURY SUV               0.0  \n",
      "3         150                LARGE PICKUP               0.0  \n",
      "4         UND  OLDER VEHICLE / UNASSIGNED               0.0  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Query all rows from the US_HY_NVR_TEST_2023_2025_REDO table\n",
    "live_query = \"\"\"\n",
    "    SELECT [REPORT_YEAR_WEEK]\n",
    "      ,[INV_STATE_ABBRV]\n",
    "      ,[INV_TOWN_NAME]\n",
    "      ,[DAYS_LSTD_BFR_SLD]\n",
    "      ,[SRC_VEH_FIRST_SCRAPED_DT]\n",
    "      ,[TRIM_DESCRIPTION]\n",
    "      ,[NVI_OWNSHP_DT]\n",
    "      ,[NVI_EFCTV_START_DT]\n",
    "      ,[NVI_CENSUS_TRACT]\n",
    "      ,[NVI_RPT_YYYYMM]\n",
    "      ,[SLS_REPORT_YEAR_MONTH]\n",
    "      ,[SALES_DT]\n",
    "      ,[SLS_CENSUS_TRACT]\n",
    "      ,[SLS_CONTROL_NBR]\n",
    "      ,[SLS_DEALER_NAME]\n",
    "      ,[SLS_TOWN_NAME]\n",
    "      ,[SLS_STATE_ABBRV]\n",
    "      ,[NVI_CONTROL_NBR]\n",
    "      ,[NVI_DEALER_NAME]\n",
    "      ,[NVI_TOWN_NAME]\n",
    "      ,[NVI_STATE_ABBRV]\n",
    "      ,[MAKE_DESC]\n",
    "      ,[MODEL_DESC]\n",
    "      ,[SERIES_TEXT]\n",
    "      ,[SEGMENT_DESC]\n",
    "      ,[SLS_VEHICLE_COUNT]\n",
    "    FROM [SPGM_Live].[SPGM_Weekly_INV_NVI_SLS_20251107]\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query and fetch all rows\n",
    "full_df_live = pd.read_sql(live_query, engine)\n",
    "\n",
    "# Display the shape and first few rows of the DataFrame\n",
    "print(f\"Total rows fetched: {len(full_df_live):,}\")\n",
    "print(full_df_live.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84b30663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of SLS_STATE_ABBRV and INV_STATE_ABBRV\n",
      "================================================================================\n",
      "Total rows: 30,625,458\n",
      "\n",
      "Rows with both values non-null: 30,625,458 (100.00%)\n",
      "\n",
      "Rows with both values non-null: 30,625,458 (100.00%)\n",
      "\n",
      "Among rows with both values:\n",
      "  Matching states: 1,862,336 (6.08%)\n",
      "  Different states: 28,763,122 (93.92%)\n",
      "\n",
      "Overall (of all 30,625,458 rows):\n",
      "  Matching states: 1,862,336 (6.08%)\n",
      "\n",
      "Among rows with both values:\n",
      "  Matching states: 1,862,336 (6.08%)\n",
      "  Different states: 28,763,122 (93.92%)\n",
      "\n",
      "Overall (of all 30,625,458 rows):\n",
      "  Matching states: 1,862,336 (6.08%)\n",
      "\n",
      "Null value counts:\n",
      "  SLS_STATE_ABBRV is null: 0 (0.00%)\n",
      "  INV_STATE_ABBRV is null: 0 (0.00%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample of mismatched states (first 10):\n",
      "\n",
      "Null value counts:\n",
      "  SLS_STATE_ABBRV is null: 0 (0.00%)\n",
      "  INV_STATE_ABBRV is null: 0 (0.00%)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Sample of mismatched states (first 10):\n",
      "SLS_STATE_ABBRV INV_STATE_ABBRV SLS_DEALER_NAME NVI_DEALER_NAME\n",
      "                             CA                                \n",
      "                             SC                                \n",
      "                             TX                    SEWELL LEXUS\n",
      "                             TX                                \n",
      "                             MT                                \n",
      "                             AZ                                \n",
      "                             CA                                \n",
      "                             FL                                \n",
      "                             TX                                \n",
      "                             TX                                \n",
      "SLS_STATE_ABBRV INV_STATE_ABBRV SLS_DEALER_NAME NVI_DEALER_NAME\n",
      "                             CA                                \n",
      "                             SC                                \n",
      "                             TX                    SEWELL LEXUS\n",
      "                             TX                                \n",
      "                             MT                                \n",
      "                             AZ                                \n",
      "                             CA                                \n",
      "                             FL                                \n",
      "                             TX                                \n",
      "                             TX                                \n"
     ]
    }
   ],
   "source": [
    "# Compare SLS_STATE_ABBRV and INV_STATE_ABBRV\n",
    "print(\"Comparison of SLS_STATE_ABBRV and INV_STATE_ABBRV\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Total rows\n",
    "total_rows = len(full_df_live)\n",
    "print(f\"Total rows: {total_rows:,}\")\n",
    "\n",
    "# Check for non-null values in both columns\n",
    "both_not_null = full_df_live['SLS_STATE_ABBRV'].notna() & full_df_live['INV_STATE_ABBRV'].notna()\n",
    "both_not_null_count = both_not_null.sum()\n",
    "\n",
    "print(f\"\\nRows with both values non-null: {both_not_null_count:,} ({100*both_not_null_count/total_rows:.2f}%)\")\n",
    "\n",
    "# Among rows where both are not null, check how many match\n",
    "if both_not_null_count > 0:\n",
    "    matching = (full_df_live.loc[both_not_null, 'SLS_STATE_ABBRV'] == \n",
    "                full_df_live.loc[both_not_null, 'INV_STATE_ABBRV'])\n",
    "    matching_count = matching.sum()\n",
    "    \n",
    "    print(f\"\\nAmong rows with both values:\")\n",
    "    print(f\"  Matching states: {matching_count:,} ({100*matching_count/both_not_null_count:.2f}%)\")\n",
    "    print(f\"  Different states: {both_not_null_count - matching_count:,} ({100*(both_not_null_count - matching_count)/both_not_null_count:.2f}%)\")\n",
    "    \n",
    "    # Overall percentage (of all rows)\n",
    "    print(f\"\\nOverall (of all {total_rows:,} rows):\")\n",
    "    print(f\"  Matching states: {matching_count:,} ({100*matching_count/total_rows:.2f}%)\")\n",
    "\n",
    "# Check null patterns\n",
    "sls_null = full_df_live['SLS_STATE_ABBRV'].isna().sum()\n",
    "inv_null = full_df_live['INV_STATE_ABBRV'].isna().sum()\n",
    "\n",
    "print(f\"\\nNull value counts:\")\n",
    "print(f\"  SLS_STATE_ABBRV is null: {sls_null:,} ({100*sls_null/total_rows:.2f}%)\")\n",
    "print(f\"  INV_STATE_ABBRV is null: {inv_null:,} ({100*inv_null/total_rows:.2f}%)\")\n",
    "\n",
    "# Show some examples of mismatches if they exist\n",
    "if both_not_null_count > 0 and matching_count < both_not_null_count:\n",
    "    print(f\"\\n{'-' * 80}\")\n",
    "    print(\"Sample of mismatched states (first 10):\")\n",
    "    mismatches = full_df_live[both_not_null & ~matching][['SLS_STATE_ABBRV', 'INV_STATE_ABBRV', 'SLS_DEALER_NAME', 'NVI_DEALER_NAME']].head(10)\n",
    "    print(mismatches.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23772171",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f992dd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INV_STATE_ABBRV Value Check:\n",
      "================================================================================\n",
      "Total rows: 30,625,458\n",
      "Null values: 0\n",
      "Empty string values: 1,640,750\n",
      "Non-null and non-empty: 28,984,708\n",
      "\n",
      "Top 10 most common values:\n",
      "INV_STATE_ABBRV\n",
      "CA    2937419\n",
      "FL    2765416\n",
      "TX    2750795\n",
      "      1640750\n",
      "NY    1439994\n",
      "PA    1343284\n",
      "NC    1305204\n",
      "MI    1206603\n",
      "OH    1171351\n",
      "VA     918279\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Quick diagnostic on INV_STATE_ABBRV values\n",
    "print(\"INV_STATE_ABBRV Value Check:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total rows: {len(full_df_live):,}\")\n",
    "print(f\"Null values: {full_df_live['INV_STATE_ABBRV'].isna().sum():,}\")\n",
    "print(f\"Empty string values: {(full_df_live['INV_STATE_ABBRV'] == '').sum():,}\")\n",
    "print(f\"Non-null and non-empty: {((full_df_live['INV_STATE_ABBRV'].notna()) & (full_df_live['INV_STATE_ABBRV'] != '')).sum():,}\")\n",
    "print(\"\\nTop 10 most common values:\")\n",
    "print(full_df_live['INV_STATE_ABBRV'].value_counts().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8034d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_make_state_sample(make, state, engine):\n",
    "    \"\"\"\n",
    "    Query data for a specific Make and State combination for sanity checking.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    make : str\n",
    "        The MAKE_DESC value to filter (e.g., 'FORD', 'CHEVROLET')\n",
    "    state : str\n",
    "        The INV_STATE_ABBRV value to filter (e.g., 'TX', 'CA')\n",
    "    engine : sqlalchemy.engine\n",
    "        Database engine connection\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        Filtered data with sales coverage analysis\n",
    "    \"\"\"\n",
    "    query = f\"\"\"\n",
    "        SELECT [REPORT_YEAR_WEEK]\n",
    "          ,[INV_STATE_ABBRV]\n",
    "          ,[DAYS_LSTD_BFR_SLD]\n",
    "          ,[SRC_VEH_FIRST_SCRAPED_DT]\n",
    "          ,[TRIM_DESCRIPTION]\n",
    "          ,[NVI_OWNSHP_DT]\n",
    "          ,[NVI_EFCTV_START_DT]\n",
    "          ,[NVI_CENSUS_TRACT]\n",
    "          ,[NVI_RPT_YYYYMM]\n",
    "          ,[SLS_REPORT_YEAR_MONTH]\n",
    "          ,[SALES_DT]\n",
    "          ,[SLS_CENSUS_TRACT]\n",
    "          ,[SLS_CONTROL_NBR]\n",
    "          ,[SLS_DEALER_NAME]\n",
    "          ,[SLS_TOWN_NAME]\n",
    "          ,[SLS_STATE_ABBRV]\n",
    "          ,[NVI_CONTROL_NBR]\n",
    "          ,[NVI_DEALER_NAME]\n",
    "          ,[NVI_TOWN_NAME]\n",
    "          ,[NVI_STATE_ABBRV]\n",
    "          ,[MAKE_DESC]\n",
    "          ,[MODEL_DESC]\n",
    "          ,[SERIES_TEXT]\n",
    "          ,[SEGMENT_DESC]\n",
    "          ,[SLS_VEHICLE_COUNT]\n",
    "        FROM [SPGM_Live].[SPGM_Weekly_INV_NVI_SLS_20251107]\n",
    "        WHERE [MAKE_DESC] = '{make}'\n",
    "          AND [INV_STATE_ABBRV] = '{state}'\n",
    "          AND [INV_STATE_ABBRV] != ''\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Querying data for: {make} in {state}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Convert SALES_DT to datetime\n",
    "    df['SALES_DT'] = pd.to_datetime(df['SALES_DT'], errors='coerce')\n",
    "    cutoff_date = pd.Timestamp('1950-01-01')\n",
    "    \n",
    "    # Calculate sales coverage\n",
    "    df['has_sales_coverage'] = (df['SALES_DT'].notna() & (df['SALES_DT'] >= cutoff_date)).astype(int)\n",
    "    \n",
    "    # Print summary\n",
    "    total = len(df)\n",
    "    with_coverage = df['has_sales_coverage'].sum()\n",
    "    coverage_pct = (with_coverage / total * 100) if total > 0 else 0\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"  Total rows: {total:,}\")\n",
    "    print(f\"  Rows with sales coverage: {with_coverage:,}\")\n",
    "    print(f\"  Coverage percentage: {coverage_pct:.1f}%\")\n",
    "    print(f\"\\nSALES_DT breakdown:\")\n",
    "    print(f\"  Null SALES_DT: {df['SALES_DT'].isna().sum():,}\")\n",
    "    print(f\"  Valid SALES_DT (>= 1950): {((df['SALES_DT'].notna()) & (df['SALES_DT'] >= cutoff_date)).sum():,}\")\n",
    "    print(f\"  Invalid SALES_DT (< 1950): {((df['SALES_DT'].notna()) & (df['SALES_DT'] < cutoff_date)).sum():,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# sample_df = query_make_state_sample('FORD', 'TX', engine)\n",
    "# print(sample_df[['MAKE_DESC', 'INV_STATE_ABBRV', 'SALES_DT', 'has_sales_coverage']].head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5a9cf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying data for: LINCOLN in AK\n",
      "================================================================================\n",
      "\n",
      "Results:\n",
      "  Total rows: 135\n",
      "  Rows with sales coverage: 0\n",
      "  Coverage percentage: 0.0%\n",
      "\n",
      "SALES_DT breakdown:\n",
      "  Null SALES_DT: 0\n",
      "  Valid SALES_DT (>= 1950): 0\n",
      "  Invalid SALES_DT (< 1950): 135\n",
      "\n",
      "Results:\n",
      "  Total rows: 135\n",
      "  Rows with sales coverage: 0\n",
      "  Coverage percentage: 0.0%\n",
      "\n",
      "SALES_DT breakdown:\n",
      "  Null SALES_DT: 0\n",
      "  Valid SALES_DT (>= 1950): 0\n",
      "  Invalid SALES_DT (< 1950): 135\n"
     ]
    }
   ],
   "source": [
    "sample_df = query_make_state_sample('LINCOLN', 'AK', engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d44a0143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Viewing first 20 rows of 135 total rows:\n",
      "================================================================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAKE_DESC</th>\n",
       "      <th>INV_STATE_ABBRV</th>\n",
       "      <th>SALES_DT</th>\n",
       "      <th>SLS_STATE_ABBRV</th>\n",
       "      <th>has_sales_coverage</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>LINCOLN</td>\n",
       "      <td>AK</td>\n",
       "      <td>1899-12-30</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    MAKE_DESC INV_STATE_ABBRV   SALES_DT SLS_STATE_ABBRV  has_sales_coverage\n",
       "0     LINCOLN              AK 1899-12-30                                   0\n",
       "1     LINCOLN              AK 1899-12-30                                   0\n",
       "2     LINCOLN              AK 1899-12-30                                   0\n",
       "3     LINCOLN              AK 1899-12-30                                   0\n",
       "4     LINCOLN              AK 1899-12-30                                   0\n",
       "..        ...             ...        ...             ...                 ...\n",
       "130   LINCOLN              AK 1899-12-30                                   0\n",
       "131   LINCOLN              AK 1899-12-30                                   0\n",
       "132   LINCOLN              AK 1899-12-30                                   0\n",
       "133   LINCOLN              AK 1899-12-30                                   0\n",
       "134   LINCOLN              AK 1899-12-30                                   0\n",
       "\n",
       "[135 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the sample data\n",
    "print(f\"\\nViewing first 20 rows of {len(sample_df):,} total rows:\")\n",
    "print(\"=\" * 80)\n",
    "sample_df[['MAKE_DESC', 'INV_STATE_ABBRV', 'SALES_DT', 'SLS_STATE_ABBRV', 'has_sales_coverage']].head(135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4cc077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Sales Coverage Analysis...\n",
      "================================================================================\n",
      "\n",
      "Data Summary:\n",
      "  Total rows: 30,625,458\n",
      "  Rows with null INV_STATE_ABBRV: 0 (0.00%)\n",
      "  Rows with empty string INV_STATE_ABBRV: 1,640,750 (5.36%)\n",
      "  Rows with null MAKE_DESC: 0 (0.00%)\n",
      "  Rows with empty string MAKE_DESC: 0 (0.00%)\n",
      "  Rows with both null: 0 (0.00%)\n",
      "  Rows used in analysis: 28,984,708 (94.64%)\n",
      "  Rows skipped: 1,640,750 (5.36%)\n",
      "\n",
      "Overall Coverage:\n",
      "  Rows with sales coverage: 271,008 (0.94%)\n",
      "  Rows without sales coverage: 28,713,700 (99.06%)\n",
      "\n",
      "Sample coverage by Make (top 5):\n",
      "             count    sum  percentage\n",
      "MAKE_DESC                            \n",
      "FORD       5522526  44868    0.812454\n",
      "CHEVROLET  3297781  30530    0.925774\n",
      "TOYOTA     2539017  20512    0.807872\n",
      "HYUNDAI    2025595  17549    0.866363\n",
      "HONDA      1959606  19118    0.975604\n",
      "\n",
      "Calculating coverage matrix...\n",
      "\n",
      "Coverage Percentage Distribution:\n",
      "================================================================================\n",
      "Total Make × State combinations: 1,692\n",
      "  0% coverage: 560 combinations (33.10%)\n",
      "  100% coverage: 0 combinations (0.00%)\n",
      "  Between 0-100%: 1,132 combinations (66.90%)\n",
      "\n",
      "Examples of partial coverage (0% < coverage < 100%):\n",
      "MAKE_DESC INV_STATE_ABBRV  total_rows  coverage_rows  coverage_percentage\n",
      "     FORD              TX      632383           4671             0.738635\n",
      "     FORD              FL      536502           5248             0.978188\n",
      "     FORD              NC      494333           3726             0.753743\n",
      "   TOYOTA              CA      463765           3562             0.768061\n",
      "     FORD              CA      358932           3037             0.846121\n",
      "     FORD              PA      352231           2919             0.828718\n",
      "CHEVROLET              FL      305043           2800             0.917903\n",
      "    HONDA              CA      293777           3269             1.112749\n",
      "CHEVROLET              MI      288958           2716             0.939929\n",
      "     FORD              MI      285080           1859             0.652098\n",
      "\n",
      "Top 20 Make × State combinations by occurrence:\n",
      "MAKE_DESC INV_STATE_ABBRV  total_rows  coverage_rows  coverage_percentage\n",
      "     FORD              TX      632383           4671                  0.7\n",
      "     FORD              FL      536502           5248                  1.0\n",
      "     FORD              NC      494333           3726                  0.8\n",
      "   TOYOTA              CA      463765           3562                  0.8\n",
      "     FORD              CA      358932           3037                  0.8\n",
      "     FORD              PA      352231           2919                  0.8\n",
      "CHEVROLET              FL      305043           2800                  0.9\n",
      "    HONDA              CA      293777           3269                  1.1\n",
      "CHEVROLET              MI      288958           2716                  0.9\n",
      "     FORD              MI      285080           1859                  0.7\n",
      "CHEVROLET              CA      264574           2609                  1.0\n",
      "CHEVROLET              TX      263016           2173                  0.8\n",
      "  HYUNDAI              FL      247917           2967                  1.2\n",
      "   TOYOTA              FL      213470           1841                  0.9\n",
      "     FORD              OH      212154           1511                  0.7\n",
      "    HONDA              FL      191022           1794                  0.9\n",
      "   NISSAN              CA      190233           1786                  0.9\n",
      "   TOYOTA              TX      189737           1949                  1.0\n",
      "     FORD              VA      187853           1776                  0.9\n",
      "  HYUNDAI              CA      184642           1669                  0.9\n",
      "\n",
      "Top 20 Make × State combinations by highest coverage percentage:\n",
      " MAKE_DESC INV_STATE_ABBRV  total_rows  coverage_rows  coverage_percentage\n",
      "   BENTLEY              OH         212             77                 36.3\n",
      "   BENTLEY              CO          53             14                 26.4\n",
      "   BENTLEY              WA          21              5                 23.8\n",
      "  MASERATI              CO          35              7                 20.0\n",
      "   BENTLEY              IN          46              9                 19.6\n",
      "     MAZDA              IA        2948            525                 17.8\n",
      "     MAZDA              ME        2143            334                 15.6\n",
      "MITSUBISHI              HI          33              5                 15.2\n",
      "   BENTLEY              MN          94             14                 14.9\n",
      "     MAZDA              RI        1143            149                 13.0\n",
      "     MAZDA              MO       16231           1975                 12.2\n",
      "     VOLVO              WV         230             27                 11.7\n",
      "  MASERATI              MO         200             21                 10.5\n",
      "     VOLVO              SD         310             32                 10.3\n",
      "     MAZDA              AL        3033            306                 10.1\n",
      "     VOLVO              SC        2729            273                 10.0\n",
      "     MAZDA              SC        4579            404                  8.8\n",
      "     MAZDA              LA        5566            462                  8.3\n",
      "  MASERATI              FL         872             72                  8.3\n",
      "     MAZDA              TX       49020           3959                  8.1\n",
      "\n",
      "✓ Matrix created: 41 Makes × 51 States\n",
      "  + Row showing % of dataset per State\n",
      "  + Column showing % of dataset per Make\n",
      "\n",
      "✓ Results exported to: sales_coverage_by_make_and_state.xlsx\n",
      "  - Sheet 1: Coverage_Matrix (percentage + counts, formatted)\n",
      "  - Sheet 2: Summary (statistics + coverage distribution)\n",
      "\n",
      "Formatting applied:\n",
      "  ✓ Column width: 20 for Makes, 18 for States\n",
      "  ✓ Row height: 20 for better readability\n",
      "  ✓ Headers: Bold with gray background\n",
      "  ✓ Column_Total row: Bold with yellow background\n",
      "  ✓ % of Dataset row/column: Bold with light blue background\n",
      "  ✓ Top 20 by occurrence: Gold background\n",
      "  ✓ Top 20 by highest coverage %: Green text\n",
      "  ✓ In both top 20 lists: Gold background + Green text\n",
      "  ✓ Center-aligned data cells\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Sales Coverage Analysis by Make and State (using INV_STATE_ABBRV)\n",
    "print(\"Preparing Sales Coverage Analysis...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define valid sales date cutoff\n",
    "cutoff_date = pd.Timestamp('1950-01-01')\n",
    "\n",
    "# Convert SALES_DT to datetime\n",
    "full_df_live['SALES_DT'] = pd.to_datetime(full_df_live['SALES_DT'], errors='coerce')\n",
    "\n",
    "# Calculate skipped rows\n",
    "total_rows = len(full_df_live)\n",
    "null_state = full_df_live['INV_STATE_ABBRV'].isna().sum()\n",
    "null_make = full_df_live['MAKE_DESC'].isna().sum()\n",
    "empty_state = (full_df_live['INV_STATE_ABBRV'] == '').sum()\n",
    "empty_make = (full_df_live['MAKE_DESC'] == '').sum()\n",
    "both_null = (full_df_live['INV_STATE_ABBRV'].isna() & full_df_live['MAKE_DESC'].isna()).sum()\n",
    "\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  Rows with null INV_STATE_ABBRV: {null_state:,} ({100*null_state/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with empty string INV_STATE_ABBRV: {empty_state:,} ({100*empty_state/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with null MAKE_DESC: {null_make:,} ({100*null_make/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with empty string MAKE_DESC: {empty_make:,} ({100*empty_make/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with both null: {both_null:,} ({100*both_null/total_rows:.2f}%)\")\n",
    "\n",
    "# Filter to valid rows (both MAKE_DESC and INV_STATE_ABBRV are not null AND not empty strings)\n",
    "valid_df = full_df_live[\n",
    "    (full_df_live['MAKE_DESC'].notna()) & \n",
    "    (full_df_live['MAKE_DESC'] != '') &\n",
    "    (full_df_live['INV_STATE_ABBRV'].notna()) & \n",
    "    (full_df_live['INV_STATE_ABBRV'] != '')\n",
    "].copy()\n",
    "valid_rows = len(valid_df)\n",
    "skipped_rows = total_rows - valid_rows\n",
    "\n",
    "print(f\"  Rows used in analysis: {valid_rows:,} ({100*valid_rows/total_rows:.2f}%)\")\n",
    "print(f\"  Rows skipped: {skipped_rows:,} ({100*skipped_rows/total_rows:.2f}%)\")\n",
    "\n",
    "# Create sales coverage flag\n",
    "valid_df['has_sales_coverage'] = (valid_df['SALES_DT'].notna() & \n",
    "                                   (valid_df['SALES_DT'] >= cutoff_date)).astype(int)\n",
    "\n",
    "# Diagnostic: Check overall distribution\n",
    "total_with_coverage = valid_df['has_sales_coverage'].sum()\n",
    "print(f\"\\nOverall Coverage:\")\n",
    "print(f\"  Rows with sales coverage: {total_with_coverage:,} ({100*total_with_coverage/valid_rows:.2f}%)\")\n",
    "print(f\"  Rows without sales coverage: {valid_rows - total_with_coverage:,} ({100*(valid_rows - total_with_coverage)/valid_rows:.2f}%)\")\n",
    "\n",
    "# Check some sample coverage percentages by Make\n",
    "print(f\"\\nSample coverage by Make (top 5):\")\n",
    "make_coverage = valid_df.groupby('MAKE_DESC')['has_sales_coverage'].agg(['count', 'sum', 'mean'])\n",
    "make_coverage['percentage'] = make_coverage['mean'] * 100\n",
    "print(make_coverage.nlargest(5, 'count')[['count', 'sum', 'percentage']])\n",
    "\n",
    "print(f\"\\nCalculating coverage matrix...\")\n",
    "\n",
    "# Create pivot table for coverage analysis\n",
    "# Group by MAKE_DESC and INV_STATE_ABBRV, calculate total rows and coverage rows\n",
    "coverage_data = valid_df.groupby(['MAKE_DESC', 'INV_STATE_ABBRV']).agg(\n",
    "    total_rows=('has_sales_coverage', 'count'),\n",
    "    coverage_rows=('has_sales_coverage', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate percentage\n",
    "coverage_data['coverage_percentage'] = (coverage_data['coverage_rows'] / coverage_data['total_rows']) * 100\n",
    "\n",
    "# Analyze coverage percentage distribution\n",
    "print(f\"\\nCoverage Percentage Distribution:\")\n",
    "print(f\"=\" * 80)\n",
    "total_combinations = len(coverage_data)\n",
    "count_0_pct = (coverage_data['coverage_percentage'] == 0).sum()\n",
    "count_100_pct = (coverage_data['coverage_percentage'] == 100).sum()\n",
    "count_between = ((coverage_data['coverage_percentage'] > 0) & (coverage_data['coverage_percentage'] < 100)).sum()\n",
    "\n",
    "print(f\"Total Make × State combinations: {total_combinations:,}\")\n",
    "print(f\"  0% coverage: {count_0_pct:,} combinations ({100*count_0_pct/total_combinations:.2f}%)\")\n",
    "print(f\"  100% coverage: {count_100_pct:,} combinations ({100*count_100_pct/total_combinations:.2f}%)\")\n",
    "print(f\"  Between 0-100%: {count_between:,} combinations ({100*count_between/total_combinations:.2f}%)\")\n",
    "\n",
    "# Show some examples of partial coverage\n",
    "if count_between > 0:\n",
    "    print(f\"\\nExamples of partial coverage (0% < coverage < 100%):\")\n",
    "    partial_coverage = coverage_data[(coverage_data['coverage_percentage'] > 0) & \n",
    "                                     (coverage_data['coverage_percentage'] < 100)].copy()\n",
    "    partial_coverage_sorted = partial_coverage.sort_values('total_rows', ascending=False).head(10)\n",
    "    print(partial_coverage_sorted[['MAKE_DESC', 'INV_STATE_ABBRV', 'total_rows', 'coverage_rows', 'coverage_percentage']].to_string(index=False))\n",
    "\n",
    "# Identify top 20 combinations by total occurrence\n",
    "top_20_combinations = coverage_data.nlargest(20, 'total_rows')[['MAKE_DESC', 'INV_STATE_ABBRV']].copy()\n",
    "top_20_set = set(zip(top_20_combinations['MAKE_DESC'], top_20_combinations['INV_STATE_ABBRV']))\n",
    "\n",
    "# Identify top 20 highest coverage percentages (excluding 0% and only for combinations with data)\n",
    "top_20_coverage = coverage_data[coverage_data['total_rows'] > 0].nlargest(20, 'coverage_percentage')[['MAKE_DESC', 'INV_STATE_ABBRV']].copy()\n",
    "top_20_coverage_set = set(zip(top_20_coverage['MAKE_DESC'], top_20_coverage['INV_STATE_ABBRV']))\n",
    "\n",
    "print(f\"\\nTop 20 Make × State combinations by occurrence:\")\n",
    "top_20_display = coverage_data.nlargest(20, 'total_rows')[['MAKE_DESC', 'INV_STATE_ABBRV', 'total_rows', 'coverage_rows', 'coverage_percentage']]\n",
    "top_20_display['coverage_percentage'] = top_20_display['coverage_percentage'].round(1)\n",
    "print(top_20_display.to_string(index=False))\n",
    "\n",
    "print(f\"\\nTop 20 Make × State combinations by highest coverage percentage:\")\n",
    "top_20_coverage_display = coverage_data[coverage_data['total_rows'] > 0].nlargest(20, 'coverage_percentage')[['MAKE_DESC', 'INV_STATE_ABBRV', 'total_rows', 'coverage_rows', 'coverage_percentage']]\n",
    "top_20_coverage_display['coverage_percentage'] = top_20_coverage_display['coverage_percentage'].round(1)\n",
    "print(top_20_coverage_display.to_string(index=False))\n",
    "\n",
    "# Create formatted string with count and percentage\n",
    "coverage_data['cell_value'] = coverage_data.apply(\n",
    "    lambda row: f\"{row['coverage_percentage']:.1f}% ({int(row['coverage_rows'])}/{int(row['total_rows'])})\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Pivot to create matrix with formatted values\n",
    "coverage_matrix_display = coverage_data.pivot(index='MAKE_DESC', \n",
    "                                               columns='INV_STATE_ABBRV', \n",
    "                                               values='cell_value')\n",
    "\n",
    "# Also create numeric matrix for calculations\n",
    "coverage_matrix_numeric = coverage_data.pivot(index='MAKE_DESC', \n",
    "                                               columns='INV_STATE_ABBRV', \n",
    "                                               values='coverage_percentage')\n",
    "\n",
    "# Fill NaN with empty string (representing 0/0 cases - no data for that combination)\n",
    "coverage_matrix_display = coverage_matrix_display.fillna('')\n",
    "coverage_matrix_numeric = coverage_matrix_numeric.fillna(0)\n",
    "\n",
    "# Calculate row totals (overall coverage per Make)\n",
    "row_totals = valid_df.groupby('MAKE_DESC').agg(\n",
    "    total_rows=('has_sales_coverage', 'count'),\n",
    "    coverage_rows=('has_sales_coverage', 'sum')\n",
    ")\n",
    "row_totals['percentage'] = (row_totals['coverage_rows'] / row_totals['total_rows']) * 100\n",
    "row_totals['formatted'] = row_totals.apply(\n",
    "    lambda row: f\"{row['percentage']:.1f}% ({int(row['coverage_rows'])}/{int(row['total_rows'])})\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate % of total dataset for each Make\n",
    "row_totals['pct_of_dataset'] = (row_totals['total_rows'] / valid_rows) * 100\n",
    "row_totals['pct_formatted'] = row_totals['pct_of_dataset'].apply(lambda x: f\"{x:.1f}%\")\n",
    "\n",
    "coverage_matrix_display['Row_Total'] = row_totals['formatted']\n",
    "coverage_matrix_display['% of Dataset'] = row_totals['pct_formatted']\n",
    "coverage_matrix_numeric['Row_Total_%'] = row_totals['percentage']\n",
    "\n",
    "# Calculate column totals (overall coverage per State)\n",
    "col_totals = valid_df.groupby('INV_STATE_ABBRV').agg(\n",
    "    total_rows=('has_sales_coverage', 'count'),\n",
    "    coverage_rows=('has_sales_coverage', 'sum')\n",
    ")\n",
    "col_totals['percentage'] = (col_totals['coverage_rows'] / col_totals['total_rows']) * 100\n",
    "\n",
    "# Calculate % of total dataset for each State\n",
    "col_totals['pct_of_dataset'] = (col_totals['total_rows'] / valid_rows) * 100\n",
    "\n",
    "# Add column totals as a new row\n",
    "col_totals_dict_display = {}\n",
    "col_totals_dict_numeric = {}\n",
    "for state in col_totals.index:\n",
    "    col_totals_dict_display[state] = f\"{col_totals.loc[state, 'percentage']:.1f}% ({int(col_totals.loc[state, 'coverage_rows'])}/{int(col_totals.loc[state, 'total_rows'])})\"\n",
    "    col_totals_dict_numeric[state] = col_totals.loc[state, 'percentage']\n",
    "\n",
    "# Grand total\n",
    "grand_total_cov = valid_df['has_sales_coverage'].sum()\n",
    "grand_total_rows = len(valid_df)\n",
    "grand_total_pct = (grand_total_cov / grand_total_rows) * 100\n",
    "col_totals_dict_display['Row_Total'] = f\"{grand_total_pct:.1f}% ({int(grand_total_cov)}/{int(grand_total_rows)})\"\n",
    "col_totals_dict_display['% of Dataset'] = \"100.0%\"\n",
    "col_totals_dict_numeric['Row_Total_%'] = grand_total_pct\n",
    "\n",
    "coverage_matrix_display.loc['Column_Total'] = col_totals_dict_display\n",
    "coverage_matrix_numeric.loc['Column_Total_%'] = col_totals_dict_numeric\n",
    "\n",
    "# Add % of Dataset row (showing what % of total dataset each state represents)\n",
    "pct_of_dataset_dict = {}\n",
    "for state in col_totals.index:\n",
    "    pct_of_dataset_dict[state] = f\"{col_totals.loc[state, 'pct_of_dataset']:.1f}%\"\n",
    "pct_of_dataset_dict['Row_Total'] = \"100.0%\"\n",
    "pct_of_dataset_dict['% of Dataset'] = \"-\"\n",
    "\n",
    "coverage_matrix_display.loc['% of Dataset'] = pct_of_dataset_dict\n",
    "\n",
    "# Sort by Row_Total_% descending (excluding the totals rows)\n",
    "sorted_makes = coverage_matrix_numeric.iloc[:-1].sort_values('Row_Total_%', ascending=False).index\n",
    "coverage_matrix_display_sorted = coverage_matrix_display.loc[sorted_makes.tolist() + ['Column_Total', '% of Dataset']]\n",
    "\n",
    "print(f\"\\n✓ Matrix created: {len(coverage_matrix_display_sorted)-2} Makes × {len(coverage_matrix_display_sorted.columns)-2} States\")\n",
    "print(f\"  + Row showing % of dataset per State\")\n",
    "print(f\"  + Column showing % of dataset per Make\")\n",
    "\n",
    "# Export to Excel with formatting\n",
    "output_file = 'sales_coverage_by_make_and_state.xlsx'\n",
    "\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Main coverage matrix with formatted values\n",
    "    coverage_matrix_display_sorted.to_excel(writer, sheet_name='Coverage_Matrix', index=True)\n",
    "    \n",
    "    # Summary statistics\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Rows in Dataset',\n",
    "            'Rows Used in Analysis',\n",
    "            'Rows Skipped (null/empty Make or State)',\n",
    "            'Percentage of Rows Skipped',\n",
    "            'Rows with null INV_STATE_ABBRV',\n",
    "            'Rows with empty INV_STATE_ABBRV',\n",
    "            'Rows with null MAKE_DESC',\n",
    "            'Rows with empty MAKE_DESC',\n",
    "            'Overall Sales Coverage %',\n",
    "            'Number of Makes',\n",
    "            'Number of States',\n",
    "            'Total Make × State Combinations',\n",
    "            'Combinations with 0% Coverage',\n",
    "            'Combinations with 100% Coverage',\n",
    "            'Combinations with Partial Coverage',\n",
    "            'Highlighted - Gold Background',\n",
    "            'Highlighted - Green Text',\n",
    "            'Highlighted - Both',\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{total_rows:,}\",\n",
    "            f\"{valid_rows:,}\",\n",
    "            f\"{skipped_rows:,}\",\n",
    "            f\"{100*skipped_rows/total_rows:.2f}%\",\n",
    "            f\"{null_state:,}\",\n",
    "            f\"{empty_state:,}\",\n",
    "            f\"{null_make:,}\",\n",
    "            f\"{empty_make:,}\",\n",
    "            f\"{grand_total_pct:.2f}%\",\n",
    "            len(coverage_matrix_display_sorted) - 2,  # Exclude both totals rows\n",
    "            len(coverage_matrix_display_sorted.columns) - 2,  # Exclude both totals columns\n",
    "            f\"{total_combinations:,}\",\n",
    "            f\"{count_0_pct:,} ({100*count_0_pct/total_combinations:.2f}%)\",\n",
    "            f\"{count_100_pct:,} ({100*count_100_pct/total_combinations:.2f}%)\",\n",
    "            f\"{count_between:,} ({100*count_between/total_combinations:.2f}%)\",\n",
    "            'Top 20 Make × State by occurrence',\n",
    "            'Top 20 Make × State by highest coverage %',\n",
    "            'Cells in both top 20 lists'\n",
    "        ]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    # Format the Coverage_Matrix sheet\n",
    "    worksheet = writer.sheets['Coverage_Matrix']\n",
    "    \n",
    "    # Set column widths\n",
    "    worksheet.column_dimensions['A'].width = 20  # MAKE_DESC column\n",
    "    for col in range(2, len(coverage_matrix_display_sorted.columns) + 2):\n",
    "        col_letter = get_column_letter(col)\n",
    "        worksheet.column_dimensions[col_letter].width = 18  # State columns\n",
    "    \n",
    "    # Set row height for better readability\n",
    "    for row in range(1, len(coverage_matrix_display_sorted) + 2):\n",
    "        worksheet.row_dimensions[row].height = 20\n",
    "    \n",
    "    # Format header row\n",
    "    header_font = Font(bold=True, size=11)\n",
    "    header_fill = PatternFill(start_color='D3D3D3', end_color='D3D3D3', fill_type='solid')\n",
    "    header_alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    for cell in worksheet[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = header_alignment\n",
    "    \n",
    "    # Format MAKE_DESC column (first column)\n",
    "    for row in range(2, len(coverage_matrix_display_sorted) + 2):\n",
    "        cell = worksheet.cell(row=row, column=1)\n",
    "        cell.font = Font(bold=True, size=10)\n",
    "        cell.alignment = Alignment(horizontal='left', vertical='center')\n",
    "    \n",
    "    # Center align all data cells\n",
    "    for row in range(2, len(coverage_matrix_display_sorted) + 2):\n",
    "        for col in range(2, len(coverage_matrix_display_sorted.columns) + 2):\n",
    "            cell = worksheet.cell(row=row, column=col)\n",
    "            cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    # Highlight top 20 combinations by occurrence (gold background) and highest coverage % (green text)\n",
    "    highlight_fill_gold = PatternFill(start_color='FFD700', end_color='FFD700', fill_type='solid')  # Gold background\n",
    "    font_green = Font(bold=True, size=10, color='00CC00')  # Bright green text, bold for emphasis\n",
    "    font_black = Font(bold=True, size=10, color='000000')  # Black text\n",
    "    \n",
    "    # Create mapping of column names to column indices\n",
    "    state_columns = [col for col in coverage_matrix_display_sorted.columns if col not in ['Row_Total', '% of Dataset']]\n",
    "    \n",
    "    # Apply highlighting based on which top 20 list(s) each cell is in\n",
    "    for make in coverage_matrix_display_sorted.index:\n",
    "        if make in ['Column_Total', '% of Dataset']:\n",
    "            continue  # Skip totals rows\n",
    "            \n",
    "        for state in state_columns:\n",
    "            combination = (make, state)\n",
    "            in_top_occurrence = combination in top_20_set\n",
    "            in_top_coverage = combination in top_20_coverage_set\n",
    "            \n",
    "            if in_top_occurrence or in_top_coverage:\n",
    "                # Find the row and column indices\n",
    "                row_idx = list(coverage_matrix_display_sorted.index).index(make) + 2\n",
    "                col_idx = state_columns.index(state) + 2\n",
    "                cell = worksheet.cell(row=row_idx, column=col_idx)\n",
    "                \n",
    "                # Apply styling based on which list(s) the combination is in\n",
    "                if in_top_occurrence and in_top_coverage:\n",
    "                    # Both: Gold background + Green text\n",
    "                    cell.fill = highlight_fill_gold\n",
    "                    cell.font = font_green\n",
    "                elif in_top_occurrence:\n",
    "                    # Only top occurrence: Gold background + Black text\n",
    "                    cell.fill = highlight_fill_gold\n",
    "                    cell.font = font_black\n",
    "                else:\n",
    "                    # Only top coverage: Green text (no background fill)\n",
    "                    cell.font = font_green\n",
    "    \n",
    "    # Format Column_Total row (second to last row)\n",
    "    column_total_row = len(coverage_matrix_display_sorted)\n",
    "    totals_fill = PatternFill(start_color='FFEB9C', end_color='FFEB9C', fill_type='solid')\n",
    "    for col in range(1, len(coverage_matrix_display_sorted.columns) + 2):\n",
    "        cell = worksheet.cell(row=column_total_row, column=col)\n",
    "        cell.fill = totals_fill\n",
    "        cell.font = Font(bold=True, size=10)\n",
    "    \n",
    "    # Format % of Dataset row (last row)\n",
    "    pct_dataset_row = len(coverage_matrix_display_sorted) + 1\n",
    "    pct_fill = PatternFill(start_color='CCE5FF', end_color='CCE5FF', fill_type='solid')  # Light blue\n",
    "    for col in range(1, len(coverage_matrix_display_sorted.columns) + 2):\n",
    "        cell = worksheet.cell(row=pct_dataset_row, column=col)\n",
    "        cell.fill = pct_fill\n",
    "        cell.font = Font(bold=True, size=10)\n",
    "    \n",
    "    # Format % of Dataset column (last column before the formatting)\n",
    "    pct_dataset_col = len(coverage_matrix_display_sorted.columns) + 1\n",
    "    for row in range(2, len(coverage_matrix_display_sorted) + 2):\n",
    "        cell = worksheet.cell(row=row, column=pct_dataset_col)\n",
    "        cell.fill = pct_fill\n",
    "        cell.font = Font(bold=True, size=10)\n",
    "    \n",
    "    # Format Summary sheet\n",
    "    summary_ws = writer.sheets['Summary']\n",
    "    summary_ws.column_dimensions['A'].width = 40\n",
    "    summary_ws.column_dimensions['B'].width = 25\n",
    "    \n",
    "    # Format header\n",
    "    for cell in summary_ws[1]:\n",
    "        cell.font = Font(bold=True, size=11)\n",
    "        cell.fill = PatternFill(start_color='D3D3D3', end_color='D3D3D3', fill_type='solid')\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "print(f\"\\n✓ Results exported to: {output_file}\")\n",
    "print(f\"  - Sheet 1: Coverage_Matrix (percentage + counts, formatted)\")\n",
    "print(f\"  - Sheet 2: Summary (statistics + coverage distribution)\")\n",
    "print(\"\\nFormatting applied:\")\n",
    "print(f\"  ✓ Column width: 20 for Makes, 18 for States\")\n",
    "print(f\"  ✓ Row height: 20 for better readability\")\n",
    "print(f\"  ✓ Headers: Bold with gray background\")\n",
    "print(f\"  ✓ Column_Total row: Bold with yellow background\")\n",
    "print(f\"  ✓ % of Dataset row/column: Bold with light blue background\")\n",
    "print(f\"  ✓ Top 20 by occurrence: Gold background\")\n",
    "print(f\"  ✓ Top 20 by highest coverage %: Green text\")\n",
    "print(f\"  ✓ In both top 20 lists: Gold background + Green text\")\n",
    "print(f\"  ✓ Center-aligned data cells\")\n",
    "print(\"\\nAnalysis complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4d69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some experiments to verify things. First: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4deb605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct INV_STATE_ABBRV Values\n",
      "================================================================================\n",
      "\n",
      "Total rows in dataframe: 30,625,458\n",
      "Null/NaN values: 0 (0.00%)\n",
      "Empty string values: 1,640,750 (5.36%)\n",
      "Non-null values: 30,625,458 (100.00%)\n",
      "\n",
      "Number of distinct values (including null): 52\n",
      "\n",
      "================================================================================\n",
      "All Distinct Values (sorted by frequency):\n",
      "================================================================================\n",
      "\n",
      "State           Count           Percentage\n",
      "--------------------------------------------------\n",
      "CA              2,937,419         9.59%\n",
      "FL              2,765,416         9.03%\n",
      "TX              2,750,795         8.98%\n",
      "                1,640,750         5.36%\n",
      "NY              1,439,994         4.70%\n",
      "PA              1,343,284         4.39%\n",
      "NC              1,305,204         4.26%\n",
      "MI              1,206,603         3.94%\n",
      "OH              1,171,351         3.82%\n",
      "VA              918,279           3.00%\n",
      "IL              916,515           2.99%\n",
      "GA              896,582           2.93%\n",
      "NJ              653,307           2.13%\n",
      "MA              608,698           1.99%\n",
      "MD              585,000           1.91%\n",
      "AZ              571,490           1.87%\n",
      "TN              524,676           1.71%\n",
      "WI              511,143           1.67%\n",
      "WA              487,619           1.59%\n",
      "MO              471,662           1.54%\n",
      "CO              455,720           1.49%\n",
      "IN              435,683           1.42%\n",
      "SC              427,434           1.40%\n",
      "AL              415,811           1.36%\n",
      "MN              390,895           1.28%\n",
      "KY              344,004           1.12%\n",
      "CT              327,455           1.07%\n",
      "OK              324,339           1.06%\n",
      "LA              323,114           1.06%\n",
      "OR              318,681           1.04%\n",
      "AR              298,454           0.97%\n",
      "IA              292,740           0.96%\n",
      "NV              285,404           0.93%\n",
      "UT              228,277           0.75%\n",
      "NH              213,123           0.70%\n",
      "KS              204,126           0.67%\n",
      "NE              169,739           0.55%\n",
      "MS              163,488           0.53%\n",
      "ID              149,506           0.49%\n",
      "WV              137,126           0.45%\n",
      "NM              134,787           0.44%\n",
      "ME              132,881           0.43%\n",
      "VT              97,318            0.32%\n",
      "DE              97,125            0.32%\n",
      "HI              94,703            0.31%\n",
      "MT              93,429            0.31%\n",
      "AK              80,656            0.26%\n",
      "RI              71,528            0.23%\n",
      "SD              68,303            0.22%\n",
      "ND              67,100            0.22%\n",
      "WY              65,910            0.22%\n",
      "PR              10,812            0.04%\n",
      "\n",
      "================================================================================\n",
      "Alphabetically sorted list of non-null states:\n",
      "================================================================================\n",
      "['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n"
     ]
    }
   ],
   "source": [
    "# Get all distinct INV_STATE_ABBRV values\n",
    "print(\"Distinct INV_STATE_ABBRV Values\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get unique values (including null/NaN)\n",
    "unique_states = full_df_live['INV_STATE_ABBRV'].unique()\n",
    "\n",
    "# Count statistics\n",
    "total_rows = len(full_df_live)\n",
    "null_count = full_df_live['INV_STATE_ABBRV'].isna().sum()\n",
    "empty_count = (full_df_live['INV_STATE_ABBRV'] == '').sum()\n",
    "non_null_count = full_df_live['INV_STATE_ABBRV'].notna().sum()\n",
    "\n",
    "print(f\"\\nTotal rows in dataframe: {total_rows:,}\")\n",
    "print(f\"Null/NaN values: {null_count:,} ({100*null_count/total_rows:.2f}%)\")\n",
    "print(f\"Empty string values: {empty_count:,} ({100*empty_count/total_rows:.2f}%)\")\n",
    "print(f\"Non-null values: {non_null_count:,} ({100*non_null_count/total_rows:.2f}%)\")\n",
    "print(f\"\\nNumber of distinct values (including null): {len(unique_states)}\")\n",
    "\n",
    "# Get value counts sorted by frequency\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"All Distinct Values (sorted by frequency):\")\n",
    "print(\"=\" * 80)\n",
    "value_counts = full_df_live['INV_STATE_ABBRV'].value_counts(dropna=False)\n",
    "print(f\"\\n{'State':<15} {'Count':<15} {'Percentage'}\")\n",
    "print(\"-\" * 50)\n",
    "for state, count in value_counts.items():\n",
    "    pct = 100 * count / total_rows\n",
    "    state_display = str(state) if pd.notna(state) else \"NaN/Null\"\n",
    "    print(f\"{state_display:<15} {count:<15,} {pct:>6.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Alphabetically sorted list of non-null states:\")\n",
    "print(\"=\" * 80)\n",
    "non_null_states = sorted([s for s in unique_states if pd.notna(s) and s != ''])\n",
    "print(non_null_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ac1b7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DE', 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME', 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY', 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69a9283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparison of SLS_STATE_ABBRV and INV_STATE_ABBRV\n",
      "================================================================================\n",
      "Total rows in dataframe: 30,625,458\n",
      "\n",
      "SLS_STATE_ABBRV:\n",
      "  Null/NaN: 0 (0.00%)\n",
      "  Empty string: 30,344,808 (99.08%)\n",
      "  Non-null and non-empty: 280,650 (0.92%)\n",
      "\n",
      "INV_STATE_ABBRV:\n",
      "  Null/NaN: 0 (0.00%)\n",
      "  Empty string: 1,640,750 (5.36%)\n",
      "  Non-null and non-empty: 28,984,708 (94.64%)\n",
      "\n",
      "================================================================================\n",
      "Analysis of rows where BOTH columns have values:\n",
      "================================================================================\n",
      "Rows with both SLS_STATE_ABBRV and INV_STATE_ABBRV present: 265,885 (0.87%)\n",
      "\n",
      "Among the 265,885 rows with both values:\n",
      "  Matching states: 236,351 (88.89%)\n",
      "  Different states: 29,534 (11.11%)\n",
      "\n",
      "================================================================================\n",
      "Sample of rows with DIFFERENT states (first 20):\n",
      "================================================================================\n",
      "SLS_STATE_ABBRV INV_STATE_ABBRV                          SLS_DEALER_NAME NVI_DEALER_NAME MAKE_DESC     MODEL_DESC\n",
      "             OH              MI                          JIM WHITE HONDA                     HONDA       PASSPORT\n",
      "             NV              CA                              GAUDIN FORD                      FORD MUSTANG MACH-E\n",
      "             VA              NC        RICK HENDRICK CHEVROLET BUICK GMC                 CHEVROLET      SILVERADO\n",
      "             NC              VA               FLOW MAZDA OF FAYETTEVILLE                     MAZDA          CX-50\n",
      "             NV              AZ                              ABC HYUNDAI                   HYUNDAI      ELANTRA N\n",
      "             MN              IA                  APPLE FORD APPLE VALLEY                      FORD       F SERIES\n",
      "             NJ              MD                  ROSSI CHEVROLET GMC INC                       GMC         SIERRA\n",
      "             TX              AZ PARK CITIES CHRYSLER DODGE JEEP RAM FIAT                  CHRYSLER       PACIFICA\n",
      "             NJ              WY                          WOODBRIDGE FORD                      FORD       EXPLORER\n",
      "             NC              SC           CROSSROADS FORD OF INDIAN TRAI                      FORD       F SERIES\n",
      "             AL              FL                        HYUNDAI OF AUBURN                   HYUNDAI        ELANTRA\n",
      "             MD              ID                            PRESTON MAZDA                      FORD         ESCAPE\n",
      "             VA              NC                             AUDI HAMPTON                   HYUNDAI         TUCSON\n",
      "             MD              VA           SHEEHY FORD OF GAITHERSBURG LC                      FORD   BRONCO SPORT\n",
      "             TX              GA             VOLVO CARS SOUTHWEST HOUSTON                     VOLVO           XC90\n",
      "             TX              VA                     PARKWAY FAMILY MAZDA                     MAZDA          CX-30\n",
      "             FL              AL                            MULLINAX FORD                      FORD       F SERIES\n",
      "             CA              NV                               AUDI MARIN                      AUDI             A6\n",
      "             CA              NV       CENTRAL VALLEY CHRYSLER JEEP DODGE                       RAM            RAM\n",
      "             AL              TX                         UNIVERSITY KIA F                       KIA       SPORTAGE\n",
      "\n",
      "================================================================================\n",
      "Most common state mismatches (Top 10):\n",
      "================================================================================\n",
      "\n",
      "SLS State    INV State    Count        % of Mismatches\n",
      "------------------------------------------------------------\n",
      "VA           MD           610            2.07%\n",
      "NJ           PA           539            1.83%\n",
      "MD           VA           515            1.74%\n",
      "FL           CA           489            1.66%\n",
      "SC           NC           473            1.60%\n",
      "MI           OH           456            1.54%\n",
      "FL           AL           447            1.51%\n",
      "NC           VA           446            1.51%\n",
      "FL           NV           444            1.50%\n",
      "NC           SC           413            1.40%\n"
     ]
    }
   ],
   "source": [
    "# Compare SLS_STATE_ABBRV and INV_STATE_ABBRV where both are present\n",
    "print(\"Comparison of SLS_STATE_ABBRV and INV_STATE_ABBRV\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total_rows = len(full_df_live)\n",
    "print(f\"Total rows in dataframe: {total_rows:,}\")\n",
    "\n",
    "# Check for null/empty values in each column\n",
    "sls_null = full_df_live['SLS_STATE_ABBRV'].isna().sum()\n",
    "sls_empty = (full_df_live['SLS_STATE_ABBRV'] == '').sum()\n",
    "inv_null = full_df_live['INV_STATE_ABBRV'].isna().sum()\n",
    "inv_empty = (full_df_live['INV_STATE_ABBRV'] == '').sum()\n",
    "\n",
    "print(f\"\\nSLS_STATE_ABBRV:\")\n",
    "print(f\"  Null/NaN: {sls_null:,} ({100*sls_null/total_rows:.2f}%)\")\n",
    "print(f\"  Empty string: {sls_empty:,} ({100*sls_empty/total_rows:.2f}%)\")\n",
    "print(f\"  Non-null and non-empty: {total_rows - sls_null - sls_empty:,} ({100*(total_rows - sls_null - sls_empty)/total_rows:.2f}%)\")\n",
    "\n",
    "print(f\"\\nINV_STATE_ABBRV:\")\n",
    "print(f\"  Null/NaN: {inv_null:,} ({100*inv_null/total_rows:.2f}%)\")\n",
    "print(f\"  Empty string: {inv_empty:,} ({100*inv_empty/total_rows:.2f}%)\")\n",
    "print(f\"  Non-null and non-empty: {total_rows - inv_null - inv_empty:,} ({100*(total_rows - inv_null - inv_empty)/total_rows:.2f}%)\")\n",
    "\n",
    "# Filter to rows where BOTH columns have non-null and non-empty values\n",
    "both_present = (\n",
    "    full_df_live['SLS_STATE_ABBRV'].notna() & \n",
    "    (full_df_live['SLS_STATE_ABBRV'] != '') &\n",
    "    full_df_live['INV_STATE_ABBRV'].notna() & \n",
    "    (full_df_live['INV_STATE_ABBRV'] != '')\n",
    ")\n",
    "both_present_count = both_present.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Analysis of rows where BOTH columns have values:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Rows with both SLS_STATE_ABBRV and INV_STATE_ABBRV present: {both_present_count:,} ({100*both_present_count/total_rows:.2f}%)\")\n",
    "\n",
    "if both_present_count > 0:\n",
    "    # Among rows where both are present, check how many match\n",
    "    matching = (\n",
    "        full_df_live.loc[both_present, 'SLS_STATE_ABBRV'] == \n",
    "        full_df_live.loc[both_present, 'INV_STATE_ABBRV']\n",
    "    )\n",
    "    matching_count = matching.sum()\n",
    "    different_count = both_present_count - matching_count\n",
    "    \n",
    "    print(f\"\\nAmong the {both_present_count:,} rows with both values:\")\n",
    "    print(f\"  Matching states: {matching_count:,} ({100*matching_count/both_present_count:.2f}%)\")\n",
    "    print(f\"  Different states: {different_count:,} ({100*different_count/both_present_count:.2f}%)\")\n",
    "    \n",
    "    # Show some examples of mismatches if they exist\n",
    "    if different_count > 0:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(f\"Sample of rows with DIFFERENT states (first 20):\")\n",
    "        print(\"=\" * 80)\n",
    "        mismatches = full_df_live[both_present & ~matching][\n",
    "            ['SLS_STATE_ABBRV', 'INV_STATE_ABBRV', 'SLS_DEALER_NAME', \n",
    "             'NVI_DEALER_NAME', 'MAKE_DESC', 'MODEL_DESC']\n",
    "        ].head(20)\n",
    "        print(mismatches.to_string(index=False))\n",
    "        \n",
    "        # Show distribution of mismatches by state pair\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"Most common state mismatches (Top 10):\")\n",
    "        print(\"=\" * 80)\n",
    "        mismatch_pairs = full_df_live[both_present & ~matching].groupby(\n",
    "            ['SLS_STATE_ABBRV', 'INV_STATE_ABBRV']\n",
    "        ).size().reset_index(name='count')\n",
    "        mismatch_pairs_sorted = mismatch_pairs.sort_values('count', ascending=False).head(10)\n",
    "        print(f\"\\n{'SLS State':<12} {'INV State':<12} {'Count':<12} {'% of Mismatches'}\")\n",
    "        print(\"-\" * 60)\n",
    "        for _, row in mismatch_pairs_sorted.iterrows():\n",
    "            pct = 100 * row['count'] / different_count\n",
    "            print(f\"{row['SLS_STATE_ABBRV']:<12} {row['INV_STATE_ABBRV']:<12} {row['count']:<12,} {pct:>6.2f}%\")\n",
    "    else:\n",
    "        print(\"\\n✓ All rows with both values present have MATCHING states!\")\n",
    "else:\n",
    "    print(\"\\n⚠ No rows found where both SLS_STATE_ABBRV and INV_STATE_ABBRV are present!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eb3801c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_sales_coverage_by_state(df, state):\n",
    "    \"\"\"\n",
    "    Check sales coverage for a specific state across all makes.\n",
    "    \n",
    "    Sales coverage is defined as:\n",
    "    - SALES_DT >= 1950-01-01 (not null and not before 1950)\n",
    "    - AND SLS_STATE_ABBRV is present (not null and not empty string)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas.DataFrame\n",
    "        The dataframe to analyze (e.g., full_df_live)\n",
    "    state : str\n",
    "        The INV_STATE_ABBRV value to filter (e.g., 'TX', 'CA')\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict with keys:\n",
    "        - state: The state analyzed\n",
    "        - total_rows: Total number of rows for this state\n",
    "        - coverage_count: Number of rows with sales coverage\n",
    "        - coverage_percentage: Percentage of rows with coverage\n",
    "        - sales_dt_valid: Number of rows with valid SALES_DT\n",
    "        - sls_state_present: Number of rows with SLS_STATE_ABBRV present\n",
    "        - filtered_df: DataFrame filtered to this state\n",
    "    \"\"\"\n",
    "    # Filter to the specified state (using INV_STATE_ABBRV)\n",
    "    state_df = df[\n",
    "        (df['INV_STATE_ABBRV'] == state) & \n",
    "        (df['INV_STATE_ABBRV'].notna()) & \n",
    "        (df['INV_STATE_ABBRV'] != '')\n",
    "    ].copy()\n",
    "    \n",
    "    total = len(state_df)\n",
    "    \n",
    "    if total == 0:\n",
    "        print(f\"\\n⚠ No data found for state: {state}\")\n",
    "        return {\n",
    "            'state': state,\n",
    "            'total_rows': 0,\n",
    "            'coverage_count': 0,\n",
    "            'coverage_percentage': 0.0,\n",
    "            'sales_dt_valid': 0,\n",
    "            'sls_state_present': 0,\n",
    "            'filtered_df': state_df\n",
    "        }\n",
    "    \n",
    "    # Convert SALES_DT to datetime if not already\n",
    "    if not pd.api.types.is_datetime64_any_dtype(state_df['SALES_DT']):\n",
    "        state_df['SALES_DT'] = pd.to_datetime(state_df['SALES_DT'], errors='coerce')\n",
    "    \n",
    "    cutoff_date = pd.Timestamp('1950-01-01')\n",
    "    \n",
    "    # Check SALES_DT status\n",
    "    sales_dt_null = state_df['SALES_DT'].isna().sum()\n",
    "    sales_dt_before_1950 = ((state_df['SALES_DT'].notna()) & (state_df['SALES_DT'] < cutoff_date)).sum()\n",
    "    sales_dt_valid = ((state_df['SALES_DT'].notna()) & (state_df['SALES_DT'] >= cutoff_date)).sum()\n",
    "    \n",
    "    # Check SLS_STATE_ABBRV status\n",
    "    sls_state_null = state_df['SLS_STATE_ABBRV'].isna().sum()\n",
    "    sls_state_empty = (state_df['SLS_STATE_ABBRV'] == '').sum()\n",
    "    sls_state_present = ((state_df['SLS_STATE_ABBRV'].notna()) & (state_df['SLS_STATE_ABBRV'] != '')).sum()\n",
    "    \n",
    "    # Calculate coverage: BOTH conditions must be true\n",
    "    has_coverage = (\n",
    "        (state_df['SALES_DT'].notna()) & \n",
    "        (state_df['SALES_DT'] >= cutoff_date) &\n",
    "        (state_df['SLS_STATE_ABBRV'].notna()) & \n",
    "        (state_df['SLS_STATE_ABBRV'] != '')\n",
    "    )\n",
    "    coverage_count = has_coverage.sum()\n",
    "    coverage_pct = (coverage_count / total * 100) if total > 0 else 0\n",
    "    \n",
    "    # Calculate non-coverage breakdown\n",
    "    no_coverage = ~has_coverage\n",
    "    no_coverage_count = no_coverage.sum()\n",
    "    \n",
    "    # Among non-coverage rows, break down by reason\n",
    "    missing_both = (\n",
    "        (state_df['SALES_DT'].isna() | (state_df['SALES_DT'] < cutoff_date)) &\n",
    "        (state_df['SLS_STATE_ABBRV'].isna() | (state_df['SLS_STATE_ABBRV'] == ''))\n",
    "    ).sum()\n",
    "    \n",
    "    missing_only_sales_dt = (\n",
    "        (state_df['SALES_DT'].isna() | (state_df['SALES_DT'] < cutoff_date)) &\n",
    "        (state_df['SLS_STATE_ABBRV'].notna() & (state_df['SLS_STATE_ABBRV'] != ''))\n",
    "    ).sum()\n",
    "    \n",
    "    missing_only_sls_state = (\n",
    "        (state_df['SALES_DT'].notna() & (state_df['SALES_DT'] >= cutoff_date)) &\n",
    "        (state_df['SLS_STATE_ABBRV'].isna() | (state_df['SLS_STATE_ABBRV'] == ''))\n",
    "    ).sum()\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nSales Coverage Analysis for State: {state}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total rows: {total:,}\")\n",
    "    \n",
    "    print(f\"\\nSALES_DT Status:\")\n",
    "    print(f\"  Null: {sales_dt_null:,} ({100*sales_dt_null/total:.1f}%)\")\n",
    "    print(f\"  Before 1950: {sales_dt_before_1950:,} ({100*sales_dt_before_1950/total:.1f}%)\")\n",
    "    print(f\"  Valid (>= 1950): {sales_dt_valid:,} ({100*sales_dt_valid/total:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\nSLS_STATE_ABBRV Status:\")\n",
    "    print(f\"  Null: {sls_state_null:,} ({100*sls_state_null/total:.1f}%)\")\n",
    "    print(f\"  Empty string: {sls_state_empty:,} ({100*sls_state_empty/total:.1f}%)\")\n",
    "    print(f\"  Present: {sls_state_present:,} ({100*sls_state_present/total:.1f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"SALES COVERAGE (SALES_DT >= 1950 AND SLS_STATE_ABBRV present):\")\n",
    "    print(f\"  With coverage: {coverage_count:,} ({coverage_pct:.1f}%)\")\n",
    "    print(f\"  Without coverage: {no_coverage_count:,} ({100*no_coverage_count/total:.1f}%)\")\n",
    "    \n",
    "    if no_coverage_count > 0:\n",
    "        print(f\"\\nBreakdown of rows WITHOUT coverage:\")\n",
    "        print(f\"  Missing both: {missing_both:,} ({100*missing_both/no_coverage_count:.1f}% of non-coverage)\")\n",
    "        print(f\"  Missing only SALES_DT: {missing_only_sales_dt:,} ({100*missing_only_sales_dt/no_coverage_count:.1f}% of non-coverage)\")\n",
    "        print(f\"  Missing only SLS_STATE_ABBRV: {missing_only_sls_state:,} ({100*missing_only_sls_state/no_coverage_count:.1f}% of non-coverage)\")\n",
    "    \n",
    "    return {\n",
    "        'state': state,\n",
    "        'total_rows': total,\n",
    "        'coverage_count': coverage_count,\n",
    "        'coverage_percentage': coverage_pct,\n",
    "        'sales_dt_valid': sales_dt_valid,\n",
    "        'sls_state_present': sls_state_present,\n",
    "        'filtered_df': state_df\n",
    "    }\n",
    "\n",
    "# Example usage:\n",
    "# result = check_sales_coverage_by_state(full_df_live, 'TX')\n",
    "# result = check_sales_coverage_by_state(full_df_live, 'CA')\n",
    "# Access the filtered dataframe: result['filtered_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ef560fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sales Coverage Analysis for State: KS\n",
      "================================================================================\n",
      "Total rows: 204,126\n",
      "\n",
      "SALES_DT Status:\n",
      "  Null: 0 (0.0%)\n",
      "  Before 1950: 202,609 (99.3%)\n",
      "  Valid (>= 1950): 1,517 (0.7%)\n",
      "\n",
      "SLS_STATE_ABBRV Status:\n",
      "  Null: 0 (0.0%)\n",
      "  Empty string: 202,662 (99.3%)\n",
      "  Present: 1,464 (0.7%)\n",
      "\n",
      "================================================================================\n",
      "SALES COVERAGE (SALES_DT >= 1950 AND SLS_STATE_ABBRV present):\n",
      "  With coverage: 1,464 (0.7%)\n",
      "  Without coverage: 202,662 (99.3%)\n",
      "\n",
      "Breakdown of rows WITHOUT coverage:\n",
      "  Missing both: 202,609 (100.0% of non-coverage)\n",
      "  Missing only SALES_DT: 0 (0.0% of non-coverage)\n",
      "  Missing only SLS_STATE_ABBRV: 53 (0.0% of non-coverage)\n"
     ]
    }
   ],
   "source": [
    "result = check_sales_coverage_by_state(full_df_live, \"KS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbf8690c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing Sales Coverage Analysis by Make and (State, Town)...\n",
      "================================================================================\n",
      "\n",
      "Data Summary:\n",
      "  Total rows: 30,625,458\n",
      "  Rows with null INV_STATE_ABBRV: 0 (0.00%)\n",
      "  Rows with empty string INV_STATE_ABBRV: 1,640,750 (5.36%)\n",
      "  Rows with null INV_TOWN_NAME: 0 (0.00%)\n",
      "  Rows with empty string INV_TOWN_NAME: 1,640,750 (5.36%)\n",
      "  Rows with null MAKE_DESC: 0 (0.00%)\n",
      "  Rows with empty string MAKE_DESC: 0 (0.00%)\n",
      "  Rows used in analysis: 28,984,708 (94.64%)\n",
      "  Rows skipped: 1,640,750 (5.36%)\n",
      "\n",
      "Overall Coverage:\n",
      "  Rows with sales coverage: 265,885 (0.92%)\n",
      "  Rows without sales coverage: 28,718,823 (99.08%)\n",
      "\n",
      "Calculating coverage by Make × (State, Town)...\n",
      "\n",
      "Coverage Statistics:\n",
      "================================================================================\n",
      "Total Make × (State, Town) combinations: 26,421\n",
      "Unique Makes: 41\n",
      "Unique States: 51\n",
      "Unique (State, Town) tuples: 4,316\n",
      "\n",
      "Coverage Distribution:\n",
      "  0% coverage: 14,217 combinations (53.81%)\n",
      "  100% coverage: 5 combinations (0.02%)\n",
      "  Between 0-100%: 12,199 combinations (46.17%)\n",
      "\n",
      "================================================================================\n",
      "Top 20 Make × (State, Town) combinations by occurrence:\n",
      "================================================================================\n",
      "MAKE_DESC          STATE_TOWN  total_rows  coverage_rows  coverage_percentage\n",
      "CHEVROLET   CA, LAKE ELSINORE       58264            549                  0.9\n",
      "CHEVROLET         CA, BANNING       58107            551                  0.9\n",
      "  HYUNDAI       NV, LAS VEGAS       52216            622                  1.2\n",
      "     FORD    FL, JACKSONVILLE       50000            267                  0.5\n",
      "   TOYOTA        CA, CERRITOS       45826            259                  0.6\n",
      "     FORD         TX, HOUSTON       44335            253                  0.6\n",
      "CHEVROLET           FL, MIAMI       42185            212                  0.5\n",
      "     FORD       NC, CHARLOTTE       32656            317                  1.0\n",
      "     FORD           FL, TAMPA       31198            290                  0.9\n",
      "     FORD FL, WEST PALM BEACH       28225            328                  1.2\n",
      "  HYUNDAI FL, NEW PORT RICHEY       27545            288                  1.0\n",
      "    HONDA         TX, HOUSTON       27234            239                  0.9\n",
      "  HYUNDAI           FL, MIAMI       26024            238                  0.9\n",
      "    HONDA    FL, JACKSONVILLE       25411            271                  1.1\n",
      "   TOYOTA          CA, ORANGE       23959            211                  0.9\n",
      "     FORD      MN, SAINT PAUL       23869            259                  1.1\n",
      "     FORD       TX, ARLINGTON       23842             34                  0.1\n",
      "     FORD         FL, HIALEAH       23707            125                  0.5\n",
      "    HONDA      OH, CINCINNATI       23270            171                  0.7\n",
      "CHEVROLET         TX, HOUSTON       22513            232                  1.0\n",
      "\n",
      "================================================================================\n",
      "Top 20 Make × (State, Town) combinations by highest coverage %:\n",
      "================================================================================\n",
      "MAKE_DESC        STATE_TOWN  total_rows  coverage_rows  coverage_percentage\n",
      " CHRYSLER   CT, NEW MILFORD          12             12                100.0\n",
      " CHRYSLER       KS, CHANUTE           2              2                100.0\n",
      " CHRYSLER      WY, EVANSTON           3              3                100.0\n",
      "    DODGE        NE, EXETER           7              7                100.0\n",
      "    DODGE         UT, PRICE           2              2                100.0\n",
      "    DODGE     WI, MENOMONIE           6              4                 66.7\n",
      "    DODGE      WY, GILLETTE          13              7                 53.8\n",
      "    DODGE    IA, SIOUX CITY           2              1                 50.0\n",
      "    DODGE    IN, MONTICELLO          14              7                 50.0\n",
      "    DODGE    NH, WOODSVILLE          14              7                 50.0\n",
      "    DODGE         OR, SALEM          28             14                 50.0\n",
      "    DODGE          PA, ERIE          14              7                 50.0\n",
      "    DODGE     WI, CUBA CITY          10              5                 50.0\n",
      "      GMC      MI, FREELAND           2              1                 50.0\n",
      "    VOLVO       PA, DU BOIS           4              2                 50.0\n",
      "  BENTLEY CA, SANTA BARBARA          27             13                 48.1\n",
      "  BENTLEY OH, NORTH OLMSTED         157             70                 44.6\n",
      " CHRYSLER    MN, LITCHFIELD          16              7                 43.8\n",
      " MASERATI  FL, CORAL GABLES          92             40                 43.5\n",
      "    MAZDA      IA, HIAWATHA         770            311                 40.4\n",
      "\n",
      "================================================================================\n",
      "Top 20 (State, Town) tuples by total occurrence (across all makes):\n",
      "================================================================================\n",
      "\n",
      "State, Town                              Total Rows      Makes      Coverage %\n",
      "--------------------------------------------------------------------------------\n",
      "TX, HOUSTON                              293,223         36            0.8%\n",
      "FL, JACKSONVILLE                         191,975         35            0.9%\n",
      "FL, MIAMI                                190,604         28            1.0%\n",
      "FL, ORLANDO                              167,351         36            1.1%\n",
      "NV, LAS VEGAS                            163,602         35            1.2%\n",
      "TX, SAN ANTONIO                          152,404         32            0.9%\n",
      "FL, TAMPA                                138,161         30            1.0%\n",
      "OH, CINCINNATI                           129,465         30            1.1%\n",
      "TX, DALLAS                               122,130         36            0.8%\n",
      "CA, CERRITOS                             101,244         30            1.0%\n",
      "TX, FORT WORTH                           99,713          30            1.0%\n",
      "NY, ROCHESTER                            94,757          31            0.9%\n",
      "OH, COLUMBUS                             92,750          27            1.0%\n",
      "NC, CHARLOTTE                            91,281          33            1.0%\n",
      "TX, AUSTIN                               90,363          35            1.1%\n",
      "AZ, PHOENIX                              88,697          33            0.6%\n",
      "MN, MINNEAPOLIS                          88,554          32            0.7%\n",
      "VA, CHESAPEAKE                           84,988          21            0.8%\n",
      "TX, ARLINGTON                            80,889          21            1.3%\n",
      "KY, LOUISVILLE                           75,252          31            0.8%\n",
      "\n",
      "✓ Analysis complete!\n",
      "  Total combinations: 26,421\n",
      "  Unique (State, Town) tuples: 4,316\n",
      "  Ready for export or further analysis\n"
     ]
    }
   ],
   "source": [
    "# Sales Coverage Analysis by Make and (State, Town) Tuple\n",
    "print(\"Preparing Sales Coverage Analysis by Make and (State, Town)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define valid sales date cutoff\n",
    "cutoff_date = pd.Timestamp('1950-01-01')\n",
    "\n",
    "# Ensure SALES_DT is datetime\n",
    "if not pd.api.types.is_datetime64_any_dtype(full_df_live['SALES_DT']):\n",
    "    full_df_live['SALES_DT'] = pd.to_datetime(full_df_live['SALES_DT'], errors='coerce')\n",
    "\n",
    "# Calculate data quality metrics\n",
    "total_rows = len(full_df_live)\n",
    "null_state = full_df_live['INV_STATE_ABBRV'].isna().sum()\n",
    "null_town = full_df_live['INV_TOWN_NAME'].isna().sum()\n",
    "null_make = full_df_live['MAKE_DESC'].isna().sum()\n",
    "empty_state = (full_df_live['INV_STATE_ABBRV'] == '').sum()\n",
    "empty_town = (full_df_live['INV_TOWN_NAME'] == '').sum()\n",
    "empty_make = (full_df_live['MAKE_DESC'] == '').sum()\n",
    "\n",
    "print(f\"\\nData Summary:\")\n",
    "print(f\"  Total rows: {total_rows:,}\")\n",
    "print(f\"  Rows with null INV_STATE_ABBRV: {null_state:,} ({100*null_state/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with empty string INV_STATE_ABBRV: {empty_state:,} ({100*empty_state/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with null INV_TOWN_NAME: {null_town:,} ({100*null_town/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with empty string INV_TOWN_NAME: {empty_town:,} ({100*empty_town/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with null MAKE_DESC: {null_make:,} ({100*null_make/total_rows:.2f}%)\")\n",
    "print(f\"  Rows with empty string MAKE_DESC: {empty_make:,} ({100*empty_make/total_rows:.2f}%)\")\n",
    "\n",
    "# Filter to valid rows (MAKE_DESC, INV_STATE_ABBRV, and INV_TOWN_NAME all not null AND not empty)\n",
    "valid_df = full_df_live[\n",
    "    (full_df_live['MAKE_DESC'].notna()) & \n",
    "    (full_df_live['MAKE_DESC'] != '') &\n",
    "    (full_df_live['INV_STATE_ABBRV'].notna()) & \n",
    "    (full_df_live['INV_STATE_ABBRV'] != '') &\n",
    "    (full_df_live['INV_TOWN_NAME'].notna()) & \n",
    "    (full_df_live['INV_TOWN_NAME'] != '')\n",
    "].copy()\n",
    "\n",
    "valid_rows = len(valid_df)\n",
    "skipped_rows = total_rows - valid_rows\n",
    "\n",
    "print(f\"  Rows used in analysis: {valid_rows:,} ({100*valid_rows/total_rows:.2f}%)\")\n",
    "print(f\"  Rows skipped: {skipped_rows:,} ({100*skipped_rows/total_rows:.2f}%)\")\n",
    "\n",
    "# Create (State, Town) tuple column\n",
    "valid_df['STATE_TOWN'] = valid_df.apply(\n",
    "    lambda row: f\"{row['INV_STATE_ABBRV']}, {row['INV_TOWN_NAME']}\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Create sales coverage flag\n",
    "valid_df['has_sales_coverage'] = (\n",
    "    (valid_df['SALES_DT'].notna()) & \n",
    "    (valid_df['SALES_DT'] >= cutoff_date) &\n",
    "    (valid_df['SLS_STATE_ABBRV'].notna()) & \n",
    "    (valid_df['SLS_STATE_ABBRV'] != '')\n",
    ").astype(int)\n",
    "\n",
    "# Overall coverage statistics\n",
    "total_with_coverage = valid_df['has_sales_coverage'].sum()\n",
    "print(f\"\\nOverall Coverage:\")\n",
    "print(f\"  Rows with sales coverage: {total_with_coverage:,} ({100*total_with_coverage/valid_rows:.2f}%)\")\n",
    "print(f\"  Rows without sales coverage: {valid_rows - total_with_coverage:,} ({100*(valid_rows - total_with_coverage)/valid_rows:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCalculating coverage by Make × (State, Town)...\")\n",
    "\n",
    "# Group by MAKE_DESC and STATE_TOWN tuple\n",
    "coverage_data = valid_df.groupby(['MAKE_DESC', 'STATE_TOWN', 'INV_STATE_ABBRV']).agg(\n",
    "    total_rows=('has_sales_coverage', 'count'),\n",
    "    coverage_rows=('has_sales_coverage', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate percentage\n",
    "coverage_data['coverage_percentage'] = (coverage_data['coverage_rows'] / coverage_data['total_rows']) * 100\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nCoverage Statistics:\")\n",
    "print(f\"=\" * 80)\n",
    "total_combinations = len(coverage_data)\n",
    "unique_towns = coverage_data['STATE_TOWN'].nunique()\n",
    "unique_states = coverage_data['INV_STATE_ABBRV'].nunique()\n",
    "unique_makes = coverage_data['MAKE_DESC'].nunique()\n",
    "\n",
    "print(f\"Total Make × (State, Town) combinations: {total_combinations:,}\")\n",
    "print(f\"Unique Makes: {unique_makes:,}\")\n",
    "print(f\"Unique States: {unique_states:,}\")\n",
    "print(f\"Unique (State, Town) tuples: {unique_towns:,}\")\n",
    "\n",
    "# Coverage distribution\n",
    "count_0_pct = (coverage_data['coverage_percentage'] == 0).sum()\n",
    "count_100_pct = (coverage_data['coverage_percentage'] == 100).sum()\n",
    "count_between = ((coverage_data['coverage_percentage'] > 0) & (coverage_data['coverage_percentage'] < 100)).sum()\n",
    "\n",
    "print(f\"\\nCoverage Distribution:\")\n",
    "print(f\"  0% coverage: {count_0_pct:,} combinations ({100*count_0_pct/total_combinations:.2f}%)\")\n",
    "print(f\"  100% coverage: {count_100_pct:,} combinations ({100*count_100_pct/total_combinations:.2f}%)\")\n",
    "print(f\"  Between 0-100%: {count_between:,} combinations ({100*count_between/total_combinations:.2f}%)\")\n",
    "\n",
    "# Top 20 combinations by occurrence\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Top 20 Make × (State, Town) combinations by occurrence:\")\n",
    "print(f\"{'='*80}\")\n",
    "top_20_display = coverage_data.nlargest(20, 'total_rows')[\n",
    "    ['MAKE_DESC', 'STATE_TOWN', 'total_rows', 'coverage_rows', 'coverage_percentage']\n",
    "]\n",
    "top_20_display['coverage_percentage'] = top_20_display['coverage_percentage'].round(1)\n",
    "print(top_20_display.to_string(index=False))\n",
    "\n",
    "# Top 20 by highest coverage percentage\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Top 20 Make × (State, Town) combinations by highest coverage %:\")\n",
    "print(f\"{'='*80}\")\n",
    "top_20_coverage_display = coverage_data[coverage_data['total_rows'] > 0].nlargest(20, 'coverage_percentage')[\n",
    "    ['MAKE_DESC', 'STATE_TOWN', 'total_rows', 'coverage_rows', 'coverage_percentage']\n",
    "]\n",
    "top_20_coverage_display['coverage_percentage'] = top_20_coverage_display['coverage_percentage'].round(1)\n",
    "print(top_20_coverage_display.to_string(index=False))\n",
    "\n",
    "# Town frequency analysis\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Top 20 (State, Town) tuples by total occurrence (across all makes):\")\n",
    "print(f\"{'='*80}\")\n",
    "town_totals = coverage_data.groupby('STATE_TOWN').agg(\n",
    "    total_rows=('total_rows', 'sum'),\n",
    "    total_coverage=('coverage_rows', 'sum'),\n",
    "    num_makes=('MAKE_DESC', 'nunique')\n",
    ").reset_index()\n",
    "town_totals['coverage_pct'] = (town_totals['total_coverage'] / town_totals['total_rows']) * 100\n",
    "town_totals_sorted = town_totals.sort_values('total_rows', ascending=False).head(20)\n",
    "print(f\"\\n{'State, Town':<40} {'Total Rows':<15} {'Makes':<10} {'Coverage %'}\")\n",
    "print(\"-\" * 80)\n",
    "for _, row in town_totals_sorted.iterrows():\n",
    "    print(f\"{row['STATE_TOWN']:<40} {int(row['total_rows']):<15,} {int(row['num_makes']):<10} {row['coverage_pct']:>6.1f}%\")\n",
    "\n",
    "print(f\"\\n✓ Analysis complete!\")\n",
    "print(f\"  Total combinations: {total_combinations:,}\")\n",
    "print(f\"  Unique (State, Town) tuples: {unique_towns:,}\")\n",
    "print(f\"  Ready for export or further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7835f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Exporting to Excel with formatting...\n",
      "================================================================================\n",
      "  Creating Top 20 by Occurrence sheet...\n",
      "  Creating Top 20 by Coverage % sheet...\n",
      "  Creating Top Towns sheet...\n",
      "  Creating All Combinations sheet...\n",
      "  Applying highlighting to All Combinations sheet...\n",
      "  Creating Summary sheet...\n",
      "\n",
      "✓ Results exported to: sales_coverage_by_make_and_state_town.xlsx\n",
      "\n",
      "Excel file contains:\n",
      "  - Sheet 1: Top 20 by Occurrence (Yellow highlighting)\n",
      "  - Sheet 2: Top 20 by Coverage % (Green highlighting)\n",
      "  - Sheet 3: Top Towns (Top 30 by occurrence)\n",
      "  - Sheet 4: All Combinations (with Yellow/Green highlighting)\n",
      "  - Sheet 5: Summary (statistics)\n",
      "\n",
      "Highlighting legend:\n",
      "  🟨 Yellow: Top 20 by occurrence\n",
      "  🟩 Green: Top 20 by highest coverage %\n",
      "  🟨 Yellow + Bold: In both top 20 lists\n",
      "\n",
      "✓ Export complete!\n"
     ]
    }
   ],
   "source": [
    "# Export Make × (State, Town) Coverage Analysis to Excel\n",
    "print(\"\\nExporting to Excel with formatting...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from openpyxl.styles import Font, Alignment, PatternFill\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "output_file = 'sales_coverage_by_make_and_state_town.xlsx'\n",
    "\n",
    "# Identify top 20 combinations by occurrence and coverage for highlighting\n",
    "top_20_occurrence = coverage_data.nlargest(20, 'total_rows')[['MAKE_DESC', 'STATE_TOWN']].copy()\n",
    "top_20_occurrence_set = set(zip(top_20_occurrence['MAKE_DESC'], top_20_occurrence['STATE_TOWN']))\n",
    "\n",
    "top_20_coverage = coverage_data[coverage_data['total_rows'] > 0].nlargest(20, 'coverage_percentage')[['MAKE_DESC', 'STATE_TOWN']].copy()\n",
    "top_20_coverage_set = set(zip(top_20_coverage['MAKE_DESC'], top_20_coverage['STATE_TOWN']))\n",
    "\n",
    "# Create formatted display data\n",
    "coverage_data_display = coverage_data.copy()\n",
    "coverage_data_display['formatted'] = coverage_data_display.apply(\n",
    "    lambda row: f\"{row['coverage_percentage']:.1f}% ({int(row['coverage_rows'])}/{int(row['total_rows'])})\", \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    # Sheet 1: Top combinations by occurrence\n",
    "    print(\"  Creating Top 20 by Occurrence sheet...\")\n",
    "    top_20_occur_export = coverage_data.nlargest(20, 'total_rows')[\n",
    "        ['MAKE_DESC', 'INV_STATE_ABBRV', 'STATE_TOWN', 'total_rows', 'coverage_rows', 'coverage_percentage']\n",
    "    ].copy()\n",
    "    top_20_occur_export.columns = ['Make', 'State', 'State, Town', 'Total Rows', 'Coverage Rows', 'Coverage %']\n",
    "    top_20_occur_export['Coverage %'] = top_20_occur_export['Coverage %'].round(1)\n",
    "    top_20_occur_export.to_excel(writer, sheet_name='Top 20 by Occurrence', index=False)\n",
    "    \n",
    "    ws1 = writer.sheets['Top 20 by Occurrence']\n",
    "    # Format header\n",
    "    header_font = Font(bold=True, size=11)\n",
    "    header_fill = PatternFill(start_color='D3D3D3', end_color='D3D3D3', fill_type='solid')\n",
    "    for cell in ws1[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    # Set column widths\n",
    "    ws1.column_dimensions['A'].width = 15\n",
    "    ws1.column_dimensions['B'].width = 10\n",
    "    ws1.column_dimensions['C'].width = 35\n",
    "    ws1.column_dimensions['D'].width = 15\n",
    "    ws1.column_dimensions['E'].width = 15\n",
    "    ws1.column_dimensions['F'].width = 12\n",
    "    # Highlight in yellow/gold\n",
    "    gold_fill = PatternFill(start_color='FFD700', end_color='FFD700', fill_type='solid')\n",
    "    for row in range(2, len(top_20_occur_export) + 2):\n",
    "        for col in range(1, 7):\n",
    "            ws1.cell(row=row, column=col).fill = gold_fill\n",
    "            ws1.cell(row=row, column=col).alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    # Sheet 2: Top combinations by coverage %\n",
    "    print(\"  Creating Top 20 by Coverage % sheet...\")\n",
    "    top_20_cov_export = coverage_data[coverage_data['total_rows'] > 0].nlargest(20, 'coverage_percentage')[\n",
    "        ['MAKE_DESC', 'INV_STATE_ABBRV', 'STATE_TOWN', 'total_rows', 'coverage_rows', 'coverage_percentage']\n",
    "    ].copy()\n",
    "    top_20_cov_export.columns = ['Make', 'State', 'State, Town', 'Total Rows', 'Coverage Rows', 'Coverage %']\n",
    "    top_20_cov_export['Coverage %'] = top_20_cov_export['Coverage %'].round(1)\n",
    "    top_20_cov_export.to_excel(writer, sheet_name='Top 20 by Coverage', index=False)\n",
    "    \n",
    "    ws2 = writer.sheets['Top 20 by Coverage']\n",
    "    # Format header\n",
    "    for cell in ws2[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    # Set column widths\n",
    "    ws2.column_dimensions['A'].width = 15\n",
    "    ws2.column_dimensions['B'].width = 10\n",
    "    ws2.column_dimensions['C'].width = 35\n",
    "    ws2.column_dimensions['D'].width = 15\n",
    "    ws2.column_dimensions['E'].width = 15\n",
    "    ws2.column_dimensions['F'].width = 12\n",
    "    # Highlight in green\n",
    "    green_fill = PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid')\n",
    "    for row in range(2, len(top_20_cov_export) + 2):\n",
    "        for col in range(1, 7):\n",
    "            ws2.cell(row=row, column=col).fill = green_fill\n",
    "            ws2.cell(row=row, column=col).alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    # Sheet 3: Top towns by occurrence\n",
    "    print(\"  Creating Top Towns sheet...\")\n",
    "    top_towns_export = town_totals.sort_values('total_rows', ascending=False).head(30).copy()\n",
    "    top_towns_export.columns = ['State, Town', 'Total Rows', 'Total Coverage', 'Num Makes', 'Coverage %']\n",
    "    top_towns_export['Coverage %'] = top_towns_export['Coverage %'].round(1)\n",
    "    top_towns_export.to_excel(writer, sheet_name='Top Towns', index=False)\n",
    "    \n",
    "    ws3 = writer.sheets['Top Towns']\n",
    "    # Format header\n",
    "    for cell in ws3[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    # Set column widths\n",
    "    ws3.column_dimensions['A'].width = 35\n",
    "    ws3.column_dimensions['B'].width = 15\n",
    "    ws3.column_dimensions['C'].width = 15\n",
    "    ws3.column_dimensions['D'].width = 12\n",
    "    ws3.column_dimensions['E'].width = 12\n",
    "    # Center align data\n",
    "    for row in range(2, len(top_towns_export) + 2):\n",
    "        for col in range(1, 6):\n",
    "            ws3.cell(row=row, column=col).alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    # Sheet 4: All combinations data\n",
    "    print(\"  Creating All Combinations sheet...\")\n",
    "    all_combinations_export = coverage_data.sort_values('total_rows', ascending=False)[\n",
    "        ['MAKE_DESC', 'INV_STATE_ABBRV', 'STATE_TOWN', 'total_rows', 'coverage_rows', 'coverage_percentage']\n",
    "    ].copy()\n",
    "    all_combinations_export.columns = ['Make', 'State', 'State, Town', 'Total Rows', 'Coverage Rows', 'Coverage %']\n",
    "    all_combinations_export['Coverage %'] = all_combinations_export['Coverage %'].round(1)\n",
    "    all_combinations_export.to_excel(writer, sheet_name='All Combinations', index=False)\n",
    "    \n",
    "    ws4 = writer.sheets['All Combinations']\n",
    "    # Format header\n",
    "    for cell in ws4[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    # Set column widths\n",
    "    ws4.column_dimensions['A'].width = 15\n",
    "    ws4.column_dimensions['B'].width = 10\n",
    "    ws4.column_dimensions['C'].width = 35\n",
    "    ws4.column_dimensions['D'].width = 15\n",
    "    ws4.column_dimensions['E'].width = 15\n",
    "    ws4.column_dimensions['F'].width = 12\n",
    "    \n",
    "    # Highlight top 20 combinations with appropriate colors\n",
    "    print(\"  Applying highlighting to All Combinations sheet...\")\n",
    "    for row_idx, row_data in enumerate(all_combinations_export.itertuples(), start=2):\n",
    "        make = row_data[1]  # Make\n",
    "        state_town = row_data[3]  # State, Town\n",
    "        combination = (make, state_town)\n",
    "        \n",
    "        in_top_occurrence = combination in top_20_occurrence_set\n",
    "        in_top_coverage = combination in top_20_coverage_set\n",
    "        \n",
    "        if in_top_occurrence and in_top_coverage:\n",
    "            # Both: Yellow background + bold\n",
    "            fill = PatternFill(start_color='FFD700', end_color='FFD700', fill_type='solid')\n",
    "            font = Font(bold=True)\n",
    "        elif in_top_occurrence:\n",
    "            # Only occurrence: Yellow background\n",
    "            fill = PatternFill(start_color='FFD700', end_color='FFD700', fill_type='solid')\n",
    "            font = Font(bold=False)\n",
    "        elif in_top_coverage:\n",
    "            # Only coverage: Green background\n",
    "            fill = PatternFill(start_color='90EE90', end_color='90EE90', fill_type='solid')\n",
    "            font = Font(bold=False)\n",
    "        else:\n",
    "            fill = None\n",
    "            font = None\n",
    "        \n",
    "        for col in range(1, 7):\n",
    "            cell = ws4.cell(row=row_idx, column=col)\n",
    "            if fill:\n",
    "                cell.fill = fill\n",
    "            if font:\n",
    "                cell.font = font\n",
    "            cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    \n",
    "    # Sheet 5: Summary statistics\n",
    "    print(\"  Creating Summary sheet...\")\n",
    "    summary_data = {\n",
    "        'Metric': [\n",
    "            'Total Rows in Dataset',\n",
    "            'Rows Used in Analysis',\n",
    "            'Rows Skipped (null/empty)',\n",
    "            'Percentage of Rows Skipped',\n",
    "            'Rows with null INV_STATE_ABBRV',\n",
    "            'Rows with empty INV_STATE_ABBRV',\n",
    "            'Rows with null INV_TOWN_NAME',\n",
    "            'Rows with empty INV_TOWN_NAME',\n",
    "            'Rows with null MAKE_DESC',\n",
    "            'Rows with empty MAKE_DESC',\n",
    "            'Overall Sales Coverage %',\n",
    "            'Number of Makes',\n",
    "            'Number of States',\n",
    "            'Number of (State, Town) tuples',\n",
    "            'Total Make × (State, Town) Combinations',\n",
    "            'Combinations with 0% Coverage',\n",
    "            'Combinations with 100% Coverage',\n",
    "            'Combinations with Partial Coverage',\n",
    "            'Top 20 by Occurrence - Yellow Highlight',\n",
    "            'Top 20 by Coverage % - Green Highlight',\n",
    "            'In Both Lists - Yellow + Bold',\n",
    "        ],\n",
    "        'Value': [\n",
    "            f\"{total_rows:,}\",\n",
    "            f\"{valid_rows:,}\",\n",
    "            f\"{skipped_rows:,}\",\n",
    "            f\"{100*skipped_rows/total_rows:.2f}%\",\n",
    "            f\"{null_state:,}\",\n",
    "            f\"{empty_state:,}\",\n",
    "            f\"{null_town:,}\",\n",
    "            f\"{empty_town:,}\",\n",
    "            f\"{null_make:,}\",\n",
    "            f\"{empty_make:,}\",\n",
    "            f\"{100*total_with_coverage/valid_rows:.2f}%\",\n",
    "            f\"{unique_makes:,}\",\n",
    "            f\"{unique_states:,}\",\n",
    "            f\"{unique_towns:,}\",\n",
    "            f\"{total_combinations:,}\",\n",
    "            f\"{count_0_pct:,} ({100*count_0_pct/total_combinations:.2f}%)\",\n",
    "            f\"{count_100_pct:,} ({100*count_100_pct/total_combinations:.2f}%)\",\n",
    "            f\"{count_between:,} ({100*count_between/total_combinations:.2f}%)\",\n",
    "            'See All Combinations sheet',\n",
    "            'See All Combinations sheet',\n",
    "            'See All Combinations sheet'\n",
    "        ]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "    \n",
    "    ws5 = writer.sheets['Summary']\n",
    "    # Format header\n",
    "    for cell in ws5[1]:\n",
    "        cell.font = header_font\n",
    "        cell.fill = header_fill\n",
    "        cell.alignment = Alignment(horizontal='center', vertical='center')\n",
    "    # Set column widths\n",
    "    ws5.column_dimensions['A'].width = 45\n",
    "    ws5.column_dimensions['B'].width = 30\n",
    "    # Align data\n",
    "    for row in range(2, len(summary_df) + 2):\n",
    "        ws5.cell(row=row, column=1).alignment = Alignment(horizontal='left', vertical='center')\n",
    "        ws5.cell(row=row, column=2).alignment = Alignment(horizontal='center', vertical='center')\n",
    "\n",
    "print(f\"\\n✓ Results exported to: {output_file}\")\n",
    "print(f\"\\nExcel file contains:\")\n",
    "print(f\"  - Sheet 1: Top 20 by Occurrence (Yellow highlighting)\")\n",
    "print(f\"  - Sheet 2: Top 20 by Coverage % (Green highlighting)\")\n",
    "print(f\"  - Sheet 3: Top Towns (Top 30 by occurrence)\")\n",
    "print(f\"  - Sheet 4: All Combinations (with Yellow/Green highlighting)\")\n",
    "print(f\"  - Sheet 5: Summary (statistics)\")\n",
    "print(f\"\\nHighlighting legend:\")\n",
    "print(f\"  🟨 Yellow: Top 20 by occurrence\")\n",
    "print(f\"  🟩 Green: Top 20 by highest coverage %\")\n",
    "print(f\"  🟨 Yellow + Bold: In both top 20 lists\")\n",
    "print(\"\\n✓ Export complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MP-ML-Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}